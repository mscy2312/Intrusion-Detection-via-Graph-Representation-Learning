{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b49903",
   "metadata": {},
   "source": [
    "## View Attack (Malicious) Time Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys, os\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _unwrap(v):\n",
    "    if isinstance(v, dict) and len(v) == 1:\n",
    "        return next(iter(v.values()))\n",
    "    return v\n",
    "\n",
    "def _iso_from_ns(ns: Optional[int]) -> Optional[str]:\n",
    "    try:\n",
    "        return datetime.fromtimestamp(int(ns) / 1_000_000_000, tz=timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _add_row(rows: List[Dict[str, Any]], name: str, start_ns: Any, end_ns: Any,\n",
    "             start_iso: Optional[str], end_iso: Optional[str], malicious: Optional[int], meta: Dict[str, Any]):\n",
    "    # Normalize ints\n",
    "    try:\n",
    "        s = int(_unwrap(start_ns)) if start_ns is not None else None\n",
    "    except Exception:\n",
    "        s = None\n",
    "    try:\n",
    "        e = int(_unwrap(end_ns)) if end_ns is not None else None\n",
    "    except Exception:\n",
    "        e = None\n",
    "\n",
    "    # Derive ISO if missing\n",
    "    if start_iso is None and s is not None:\n",
    "        start_iso = _iso_from_ns(s)\n",
    "    if end_iso is None and e is not None:\n",
    "        end_iso = _iso_from_ns(e)\n",
    "\n",
    "    # Label: default attacks => malicious=1 unless explicitly given 0/False\n",
    "    lbl = malicious\n",
    "    if lbl is None:\n",
    "        lbl = 1  # every interval in this file is an attack window by definition\n",
    "\n",
    "    rows.append({\n",
    "        \"name\": name,\n",
    "        \"start_ns\": s,\n",
    "        \"end_ns\": e,\n",
    "        \"start_iso\": start_iso,\n",
    "        \"end_iso\": end_iso,\n",
    "        \"malicious\": int(bool(lbl)),\n",
    "        **meta\n",
    "    })\n",
    "\n",
    "# ---------- main loader ----------\n",
    "def load_attack_windows(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Schema A: {\"attacks_by_host\": {\"HOST\": [ {start_ns, end_ns, ...}, ... ], ...}}\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"attacks_by_host\"), dict):\n",
    "        for host, arr in data[\"attacks_by_host\"].items():\n",
    "            if isinstance(arr, list):\n",
    "                for i, a in enumerate(arr):\n",
    "                    if not isinstance(a, dict): \n",
    "                        continue\n",
    "                    start_ns = a.get(\"start_ns\") or (a.get(\"window\") or {}).get(\"start_unix_ns\")\n",
    "                    end_ns   = a.get(\"end_ns\")   or (a.get(\"window\") or {}).get(\"end_unix_ns\")\n",
    "                    start_iso = a.get(\"start_iso\") or (a.get(\"window\") or {}).get(\"start_iso\")\n",
    "                    end_iso   = a.get(\"end_iso\")   or (a.get(\"window\") or {}).get(\"end_iso\")\n",
    "                    malicious = a.get(\"malicious\")\n",
    "                    name = a.get(\"name\") or a.get(\"label\") or host\n",
    "                    _add_row(rows, name, start_ns, end_ns, start_iso, end_iso, malicious,\n",
    "                             {\"host\": host, \"idx\": i})\n",
    "        return rows\n",
    "\n",
    "    # Schema B: {\"attacks\": [ {\"window\":{\"start_unix_ns\",\"end_unix_ns\",\"start_iso\",\"end_iso\"}, \"name\":...,\"malicious\":...}, ... ]}\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"attacks\"), list):\n",
    "        for i, a in enumerate(data[\"attacks\"]):\n",
    "            if not isinstance(a, dict): \n",
    "                continue\n",
    "            w = a.get(\"window\", {})\n",
    "            start_ns = w.get(\"start_unix_ns\")\n",
    "            end_ns   = w.get(\"end_unix_ns\")\n",
    "            start_iso = w.get(\"start_iso\")\n",
    "            end_iso   = w.get(\"end_iso\")\n",
    "            malicious = a.get(\"malicious\")\n",
    "            name = a.get(\"name\") or a.get(\"label\") or f\"attack_{i}\"\n",
    "            _add_row(rows, name, start_ns, end_ns, start_iso, end_iso, malicious, {\"idx\": i})\n",
    "        return rows\n",
    "\n",
    "    # Schema C: {\"intervals\": [[start_ns, end_ns], ...]}\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"intervals\"), list):\n",
    "        for i, pair in enumerate(data[\"intervals\"]):\n",
    "            if isinstance(pair, (list, tuple)) and len(pair) == 2:\n",
    "                _add_row(rows, f\"attack_{i}\", pair[0], pair[1], None, None, 1, {\"idx\": i})\n",
    "        return rows\n",
    "\n",
    "    # Schema D: list of dicts like [{\"start_ns\":..., \"end_ns\":..., \"start_iso\":..., \"end_iso\":..., \"name\":...}, ...]\n",
    "    if isinstance(data, list):\n",
    "        for i, a in enumerate(data):\n",
    "            if not isinstance(a, dict):\n",
    "                continue\n",
    "            start_ns = a.get(\"start_ns\") or (a.get(\"window\") or {}).get(\"start_unix_ns\")\n",
    "            end_ns   = a.get(\"end_ns\")   or (a.get(\"window\") or {}).get(\"end_unix_ns\")\n",
    "            start_iso = a.get(\"start_iso\") or (a.get(\"window\") or {}).get(\"start_iso\")\n",
    "            end_iso   = a.get(\"end_iso\")   or (a.get(\"window\") or {}).get(\"end_iso\")\n",
    "            malicious = a.get(\"malicious\")\n",
    "            name = a.get(\"name\") or a.get(\"label\") or f\"attack_{i}\"\n",
    "            _add_row(rows, name, start_ns, end_ns, start_iso, end_iso, malicious, {\"idx\": i})\n",
    "        return rows\n",
    "\n",
    "    # Fallback: unknown structure\n",
    "    raise ValueError(\"Unsupported attack windows schema in file: \" + path)\n",
    "\n",
    "# ---------- main ----------\n",
    "path = r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\attack_windows_e5_iso_and_ns_cleaned.json\"\n",
    "if not os.path.isfile(path):\n",
    "    print(f\"[ERROR] File not found: {path}\")\n",
    "else:\n",
    "    rows = load_attack_windows(path)\n",
    "\n",
    "    # Sort by start_ns (if present)\n",
    "    rows.sort(key=lambda r: (r[\"start_ns\"] is None, r[\"start_ns\"]))\n",
    "\n",
    "    # Show summary\n",
    "    total = len(rows)\n",
    "    mal = sum(r.get(\"malicious\", 0) == 1 for r in rows)\n",
    "    print(f\"\\nLoaded attack windows: {total} | malicious=1 count: {mal}\\n\")\n",
    "\n",
    "    # Pretty display\n",
    "    cols = [\"name\", \"start_iso\", \"end_iso\", \"start_ns\", \"end_ns\", \"malicious\"]\n",
    "    if pd:\n",
    "        df = pd.DataFrame(rows)\n",
    "        # If 'host' exists (in some schemas), include it\n",
    "        if \"host\" in df.columns and \"host\" not in cols:\n",
    "            cols.insert(1, \"host\")\n",
    "        print(df[ [c for c in cols if c in df.columns] ].to_string(index=False))\n",
    "    else:\n",
    "        # fallback plain text table\n",
    "        def fmt(x): return \"\" if x is None else str(x)\n",
    "        header = \" | \".join(cols)\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "        for r in rows:\n",
    "            print(\" | \".join(fmt(r.get(c)) for c in cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c4025",
   "metadata": {},
   "source": [
    "## Check Malicious Events in Malicious Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "malicious_dir = Path(r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\out_parquet_time_only\\events\\malicious\")\n",
    "\n",
    "total_malicious = 0\n",
    "results = {}\n",
    "\n",
    "for parquet_file in malicious_dir.glob(\"*.parquet\"):\n",
    "    df = pd.read_parquet(parquet_file, columns=[\"malicious\"])\n",
    "    \n",
    "    # normalize column\n",
    "    s = pd.to_numeric(df[\"malicious\"], errors=\"coerce\")\n",
    "    \n",
    "    cnt_malicious = (s == 1).sum()\n",
    "    cnt_non = (s != 1).sum()\n",
    "    \n",
    "    results[parquet_file.name] = {\"malicious\": int(cnt_malicious), \"non_malicious\": int(cnt_non)}\n",
    "    total_malicious += int(cnt_malicious)\n",
    "\n",
    "# Print per-file summary\n",
    "print(\"=== Malicious Folder File-by-File Check ===\")\n",
    "for fname, counts in results.items():\n",
    "    print(f\"{fname}: malicious={counts['malicious']}, non_malicious={counts['non_malicious']}\")\n",
    "\n",
    "print(\"\\nTOTAL malicious events:\", total_malicious)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d7ea0",
   "metadata": {},
   "source": [
    "## Check Non-Malicious Events in Non-Malicious Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6538ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "non_malicious_dir = Path(r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\out_parquet_time_only\\events\\non_malicious\")\n",
    "\n",
    "total_non_malicious = 0\n",
    "results = {}\n",
    "\n",
    "for parquet_file in non_malicious_dir.glob(\"*.parquet\"):\n",
    "    df = pd.read_parquet(parquet_file, columns=[\"malicious\"])\n",
    "    \n",
    "    # normalize column\n",
    "    s = pd.to_numeric(df[\"malicious\"], errors=\"coerce\")\n",
    "    \n",
    "    cnt_non = (s == 0).sum()\n",
    "    cnt_malicious = (s != 0).sum()\n",
    "    \n",
    "    results[parquet_file.name] = {\"non_malicious\": int(cnt_non), \"malicious\": int(cnt_malicious)}\n",
    "    total_non_malicious += int(cnt_non)\n",
    "\n",
    "# Print per-file summary\n",
    "print(\"=== Non-Malicious Folder File-by-File Check ===\")\n",
    "for fname, counts in results.items():\n",
    "    print(f\"{fname}: non_malicious={counts['non_malicious']}, malicious={counts['malicious']}\")\n",
    "\n",
    "print(\"\\nTOTAL non-malicious events:\", total_non_malicious)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897210f",
   "metadata": {},
   "source": [
    "## Inspect Features for each node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50355a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inspect_features.py\n",
    "\n",
    "Explore what columns (features) exist for each node type (subjects, fileobjects, etc.)\n",
    "and for events in the preprocessed Parquet dataset. Also prints a few sample rows.\n",
    "\n",
    "Usage:\n",
    "  python inspect_features.py --input-dir ./out_parquet_time_only --samples 3\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def inspect_folder(parquet_dir: Path, n_samples: int = 3):\n",
    "    \"\"\"Inspect schema and sample rows from a parquet folder.\"\"\"\n",
    "    files = sorted(parquet_dir.glob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        return None, None\n",
    "    # Open first file\n",
    "    pf = pq.ParquetFile(files[0])\n",
    "    cols = pf.schema.names\n",
    "    # Read a few rows as dicts\n",
    "    table = pf.read_row_groups([0], columns=cols)\n",
    "    rows = table.to_pylist()[:n_samples]\n",
    "    return cols, rows\n",
    "\n",
    "#------ main -----------\n",
    "input_dir = r\"./out_parquet_time_only\"\n",
    "samples = 2\n",
    "base = Path(input_dir)\n",
    "if not base.exists():\n",
    "    raise FileNotFoundError(f\"{base} not found\")\n",
    "\n",
    "folders = [p for p in base.iterdir() if p.is_dir()]\n",
    "print(f\"Inspecting {len(folders)} parquet folders under {base}\\n\")\n",
    "\n",
    "for folder in folders:\n",
    "    cols, rows = inspect_folder(folder, samples)\n",
    "    if cols is None:\n",
    "        print(f\"[{folder.name}] No parquet files found\\n\")\n",
    "        continue\n",
    "\n",
    "    print(folder.name.upper())\n",
    "    print(f\"Total Columns: {len(cols)}\")\n",
    "    print(\"Columns: \" + \", \".join(cols))\n",
    "    for i, row in enumerate(rows):\n",
    "        print(f\"Sample Row {i+1}: {row}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b9093",
   "metadata": {},
   "source": [
    "## Make 10-Mint Window with UUID to Type --> .gpickle Output\n",
    "\n",
    "Quick test script to extract a single 10-minute provenance graph\n",
    "from your events parquet folder. Saves to .gpickle and prints:\n",
    "- number of nodes\n",
    "- number of edges\n",
    "- top-degree nodes\n",
    "- sample edges\n",
    "- time taken\n",
    "\n",
    "Now includes UUID → type mapping so nodes get real types\n",
    "(subjects, fileobjects, netflows, etc.) instead of 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ----------- Config -----------\n",
    "BASE_DIR = Path(\"./out_parquet_time_only\")\n",
    "INPUT_DIR = BASE_DIR / \"events\"\n",
    "OUTPUT_FILE = Path(\"./test_window.gpickle\")\n",
    "WINDOW_MIN = 10\n",
    "BATCH_SIZE = 100_000\n",
    "LOG_LEVEL = logging.INFO\n",
    "# ------------------------------\n",
    "\n",
    "NS_PER_SEC = 1_000_000_000\n",
    "NS_PER_MIN = 60 * NS_PER_SEC\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(levelname)s %(asctime)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    level=LOG_LEVEL\n",
    ")\n",
    "log = logging.getLogger(\"test-one-window\")\n",
    "\n",
    "def ns_to_iso(ns: int) -> str:\n",
    "    return datetime.fromtimestamp(ns / 1e9, tz=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def save_graph(G, path: Path):\n",
    "    if hasattr(nx, \"write_gpickle\"):\n",
    "        nx.write_gpickle(G, path)\n",
    "    else:\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(G, f)\n",
    "\n",
    "def build_uuid_type_map(base: Path) -> dict:\n",
    "    \"\"\"Scan non-event folders (subjects, fileobjects, etc.) and build UUID → type map.\"\"\"\n",
    "    uuid2type = {}\n",
    "    node_folders = [p for p in base.iterdir() if p.is_dir() and p.name.lower() != \"events\"]\n",
    "    for folder in node_folders:\n",
    "        try:\n",
    "            dataset = ds.dataset(folder, format=\"parquet\")\n",
    "        except Exception as e:\n",
    "            log.warning(f\"Skipping {folder}: {e}\")\n",
    "            continue\n",
    "        for frag in dataset.get_fragments():\n",
    "            try:\n",
    "                tb = pq.read_table(frag.path, columns=[\"uuid\"])\n",
    "            except Exception as e:\n",
    "                log.warning(f\"Error reading {frag.path}: {e}\")\n",
    "                continue\n",
    "            for row in tb.to_pylist():\n",
    "                u = row.get(\"uuid\")\n",
    "                if u and u not in uuid2type:\n",
    "                    uuid2type[u] = folder.name\n",
    "    log.info(f\"UUID→type map built with {len(uuid2type):,} entries\")\n",
    "    return uuid2type\n",
    "\n",
    "# ----------- main -----------\n",
    "win_ns = WINDOW_MIN * NS_PER_MIN\n",
    "graphs = {}\n",
    "\n",
    "t0 = time.time()\n",
    "files = sorted(INPUT_DIR.glob(\"*.parquet\"))\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No parquet files in events/ folder\")\n",
    "\n",
    "log.info(f\"Found {len(files)} parquet files, using first to build one window\")\n",
    "\n",
    "# Build UUID→type map first\n",
    "log.info(f\"START: Building UUID→type map from node folders...\")\n",
    "uuid2type = build_uuid_type_map(BASE_DIR)\n",
    "log.info(f\"Completed: Building UUID→type map\")\n",
    "\n",
    "for file in files:\n",
    "    log.info(f\"Reading file {file.name}\")\n",
    "    pf = pq.ParquetFile(file)\n",
    "    cols = [c for c in [\"timestampNanos\",\"event_type\",\"subject_uuid\",\"object1_uuid\",\"object2_uuid\",\"malicious\"]\n",
    "            if c in pf.schema_arrow.names]\n",
    "\n",
    "    for batch in pf.iter_batches(batch_size=BATCH_SIZE, columns=cols):\n",
    "        rows = batch.to_pylist()\n",
    "        for r in rows:\n",
    "            ts = r.get(\"timestampNanos\")\n",
    "            if ts is None: \n",
    "                continue\n",
    "            ts = int(ts)\n",
    "            ws = (ts // win_ns) * win_ns\n",
    "            if ws not in graphs:\n",
    "                graphs[ws] = nx.MultiDiGraph()\n",
    "            u, v1, v2 = r.get(\"subject_uuid\"), r.get(\"object1_uuid\"), r.get(\"object2_uuid\")\n",
    "            for node in [u,v1,v2]:\n",
    "                if node and not graphs[ws].has_node(node):\n",
    "                    graphs[ws].add_node(node, node_type=uuid2type.get(node,\"unknown\"))\n",
    "            et = r.get(\"event_type\")\n",
    "            mal = bool(r.get(\"malicious\", False))\n",
    "            if u and v1:\n",
    "                graphs[ws].add_edge(u,v1,event_type=et,timestamp=ts,malicious=mal)\n",
    "            if u and v2:\n",
    "                graphs[ws].add_edge(u,v2,event_type=et,timestamp=ts,malicious=mal)\n",
    "\n",
    "        # Once first window fills, flush and exit\n",
    "        if graphs:\n",
    "            ws0 = min(graphs.keys())\n",
    "            G = graphs[ws0]\n",
    "            save_graph(G, OUTPUT_FILE)\n",
    "            elapsed = time.time() - t0\n",
    "            log.info(f\"Saved {OUTPUT_FILE}\")\n",
    "            log.info(f\"Window start: {ns_to_iso(ws0)}\")\n",
    "            log.info(f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "            log.info(f\"Time taken: {elapsed:.2f} sec\")\n",
    "\n",
    "            # ---- Visual inspection ----\n",
    "            log.info(\"Top 10 nodes by degree:\")\n",
    "            degs = sorted(G.degree, key=lambda x: x[1], reverse=True)[:10]\n",
    "            for node, d in degs:\n",
    "                log.info(f\"  {node} ({G.nodes[node]['node_type']}) -> degree {d}\")\n",
    "\n",
    "            log.info(\"Sample edges (3):\")\n",
    "            for i, (u,v,edata) in enumerate(G.edges(data=True)):\n",
    "                if i >= 3: break\n",
    "                log.info(f\"  {u} ({G.nodes[u]['node_type']}) -> {v} ({G.nodes[v]['node_type']}), attrs={edata}\")\n",
    "            break # stop after first window\n",
    "    break  # stop after first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea66b64",
   "metadata": {},
   "source": [
    "## Visualize a small subgraph for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def load_graph(path: Path):\n",
    "    if hasattr(nx, \"read_gpickle\"):\n",
    "        return nx.read_gpickle(path)\n",
    "    else:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "# Arguments\n",
    "gpickle_path = Path(\"test_window_withUUID_to_Type.gpickle\")\n",
    "# gpickle_path = Path(\"test_window_withoutUUID_to_Type.gpickle\")\n",
    "max_nodes = 50\n",
    "\n",
    "G = load_graph(Path(gpickle_path))\n",
    "print(f\"Loaded graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Pick top-degree nodes to keep\n",
    "nodes_sorted = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
    "keep_nodes = [n for n, _ in nodes_sorted[:max_nodes]]\n",
    "H = G.subgraph(keep_nodes).copy()\n",
    "\n",
    "print(f\"Visualizing subgraph with {H.number_of_nodes()} nodes and {H.number_of_edges()} edges\")\n",
    "\n",
    "# Node colors by type\n",
    "node_types = nx.get_node_attributes(H, \"node_type\")\n",
    "color_map = {\"subjects\": \"skyblue\", \"fileobjects\": \"orange\",\n",
    "                \"netflows\": \"green\", \"memory\": \"purple\",\n",
    "                \"ipc\": \"pink\", \"registrykeys\": \"red\", \"unknown\": \"gray\"}\n",
    "node_colors = [color_map.get(node_types.get(n, \"unknown\"), \"gray\") for n in H.nodes]\n",
    "\n",
    "pos = nx.spring_layout(H, seed=42, k=0.3)  # spring layout\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw_networkx_nodes(H, pos, node_size=300, node_color=node_colors)\n",
    "nx.draw_networkx_edges(H, pos, alpha=0.4)\n",
    "nx.draw_networkx_labels(H, pos, font_size=7)\n",
    "\n",
    "plt.title(f\"Subgraph of {gpickle_path} (top {max_nodes} nodes by degree)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcdbe1",
   "metadata": {},
   "source": [
    "## Check Edge Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_edge_dims.py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\win_tensors_balanced\")\n",
    "pt_files = list(data_dir.glob(\"*.pt\")) # Check all files\n",
    "\n",
    "for pt_file in pt_files:\n",
    "    data = torch.load(pt_file, weights_only=False) # Adjust if your data is a Data object directly\n",
    "    # If data is a dict-like object from torch.load\n",
    "    if isinstance(data, dict):\n",
    "        edge_attr = data.get('edge_attr')\n",
    "    # If data is a PyG Data object directly (less common with torch.save dict)\n",
    "    # else:\n",
    "    #     edge_attr = getattr(data, 'edge_attr', None)\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        print(f\"{pt_file.name}: edge_attr.shape = {edge_attr.shape}\")\n",
    "        # Check if the number of features (columns) is consistent (e.g., 63)\n",
    "        # assert edge_attr.shape[1] == 63, f\"Inconsistent dim in {pt_file.name}\"\n",
    "    else:\n",
    "        print(f\"{pt_file.name}: edge_attr not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b498746",
   "metadata": {},
   "source": [
    "## Check Graph Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13285477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_graph_dims.py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Point this to your directory containing the .pt files\n",
    "data_dir = Path(r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\win_tensors_balanced\")\n",
    "\n",
    "# Get a list of .pt files (checking first few for speed, you can remove the slice [:10] to check all)\n",
    "pt_files = list(data_dir.glob(\"*.pt\")) #[:10] # Remove [:10] to check all files\n",
    "\n",
    "# Use a set to store unique numbers of node features found\n",
    "unique_x_dims = set()\n",
    "\n",
    "print(f\"Checking dimensions for node features (x) in .pt files...\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for pt_file in pt_files:\n",
    "    try:\n",
    "        # Load the .pt file. It likely contains a dictionary.\n",
    "        data_dict = torch.load(pt_file, weights_only=False)\n",
    "\n",
    "        # Extract the node feature tensor 'x'\n",
    "        # Adjust the key name if your script saves it differently (e.g., 'node_features')\n",
    "        x = data_dict.get('x')\n",
    "\n",
    "        if x is not None:\n",
    "            # Get the number of features (columns) for nodes\n",
    "            num_node_features = x.shape[1] # Shape is typically [num_nodes, num_features]\n",
    "            unique_x_dims.add(num_node_features)\n",
    "            # Print the first few files' dimensions to see an example\n",
    "            if len(unique_x_dims) <= 3: # Just print a few examples\n",
    "                 print(f\"{pt_file.name}: x.shape = {x.shape}\")\n",
    "        else:\n",
    "            print(f\"{pt_file.name}: 'x' tensor not found in the loaded data.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pt_file.name}: {e}\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(f\"Unique node feature dimensions (x.shape[1]) found: {sorted(unique_x_dims)}\")\n",
    "\n",
    "if len(unique_x_dims) > 1:\n",
    "    print(\"\\n*** INCONSISTENCY DETECTED ***\")\n",
    "    print(\"Graphs have different numbers of node features.\")\n",
    "    print(\"This will cause a batching error in gine_train.py.\")\n",
    "else:\n",
    "    print(\"\\nNode feature dimensions appear consistent (based on checked files).\")\n",
    "    print(\"If gine_train.py still fails, the issue might be elsewhere or in unchecked files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebef7a",
   "metadata": {},
   "source": [
    "## Analyze TCCDMDatum.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd10b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def walk_schema(obj, found):\n",
    "    \"\"\"Recursively walk JSON schema and collect record/enum names.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        t = obj.get(\"type\")\n",
    "        name = obj.get(\"name\")\n",
    "\n",
    "        # If it's an enum, record its symbols\n",
    "        if t == \"enum\" and name:\n",
    "            found[\"enums\"][name] = obj.get(\"symbols\", [])\n",
    "\n",
    "        # If it's a record, record its fields\n",
    "        if t == \"record\" and name:\n",
    "            found[\"records\"].append(name)\n",
    "            for f in obj.get(\"fields\", []):\n",
    "                walk_schema(f.get(\"type\"), found)\n",
    "\n",
    "        # Unions or nested types\n",
    "        if isinstance(t, list):\n",
    "            for u in t:\n",
    "                walk_schema(u, found)\n",
    "\n",
    "        # Recurse into dicts\n",
    "        if isinstance(t, dict):\n",
    "            walk_schema(t, found)\n",
    "\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            walk_schema(item, found)\n",
    "\n",
    "# ----------- main -----------\n",
    "schema = r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\TCCDMDatum.json\"\n",
    "path = Path(schema)\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "found = {\"records\": [], \"enums\": {}}\n",
    "walk_schema(schema, found)\n",
    "\n",
    "print(\"=== Record Types ===\")\n",
    "for r in sorted(set(found[\"records\"])):\n",
    "    print(\" -\", r)\n",
    "\n",
    "print(\"\\n=== Enums and Symbols ===\")\n",
    "for enum, symbols in found[\"enums\"].items():\n",
    "    print(f\"{enum}: {symbols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df1b5e",
   "metadata": {},
   "source": [
    "## Analyze .pt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inspect_pt_file.py\n",
    "\n",
    "Inspect a .pt file from your DARPA E5 dataset to diagnose KeyError in gine_train.py.\n",
    "Prints top-level keys and detailed contents (tensor shapes, dict structures).\n",
    "Optionally scans multiple files for key consistency.\n",
    "\n",
    "Usage:\n",
    "  python inspect_pt_file.py --pt-file \"path/to/sample.pt\"\n",
    "  # or scan all .pt files in a directory:\n",
    "  python inspect_pt_file.py --data-dir \"path/to/win_tensors_balanced\"\n",
    "\n",
    "Requires:\n",
    "  - Python 3.10+\n",
    "  - PyTorch >= 2.2\n",
    "  - PyTorch Geometric >= 2.5\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def print_tensor_info(name: str, tensor: Any) -> None:\n",
    "    \"\"\"Print tensor shape and dtype, or value if scalar.\"\"\"\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        print(f\"  {name}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {name}: {tensor} (type={type(tensor).__name__})\")\n",
    "\n",
    "def inspect_pt_file(pt_path: Path) -> None:\n",
    "    \"\"\"Inspect a single .pt file's contents.\"\"\"\n",
    "    print(f\"\\nInspecting: {pt_path.name}\")\n",
    "    try:\n",
    "        obj = torch.load(pt_path, map_location=\"cpu\")\n",
    "        print(\"Top-level keys:\", list(obj.keys()) if isinstance(obj, dict) else \"PyG Data object\")\n",
    "        \n",
    "        if isinstance(obj, Data):\n",
    "            # Handle PyG Data object\n",
    "            print(\"Attributes in Data object:\")\n",
    "            for key, value in obj.items():\n",
    "                print_tensor_info(key, value)\n",
    "        elif isinstance(obj, dict):\n",
    "            # Handle raw dict\n",
    "            print(\"Contents of dict:\")\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"  {key}: dict with keys {list(value.keys())}\")\n",
    "                else:\n",
    "                    print_tensor_info(key, value)\n",
    "        else:\n",
    "            print(f\"Unexpected type: {type(obj).__name__}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {pt_path.name}: {str(e)}\")\n",
    "\n",
    "def scan_directory(data_dir: Path) -> None:\n",
    "    \"\"\"Scan all .pt files in a directory for key consistency.\"\"\"\n",
    "    pt_files = sorted([p for p in data_dir.glob(\"*.pt\") if p.is_file()])\n",
    "    if not pt_files:\n",
    "        print(f\"No .pt files found in {data_dir}\")\n",
    "        return\n",
    "    \n",
    "    key_sets: Dict[str, set] = {}\n",
    "    for pt_file in pt_files:\n",
    "        try:\n",
    "            obj = torch.load(pt_file, map_location=\"cpu\")\n",
    "            keys = set(obj.keys()) if isinstance(obj, dict) else set(obj.keys)\n",
    "            key_sets[pt_file.name] = keys\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {pt_file.name}: {str(e)}\")\n",
    "    \n",
    "    # Check for inconsistent keys\n",
    "    print(\"\\nKey consistency across files:\")\n",
    "    all_keys = set.union(*key_sets.values())\n",
    "    for key in all_keys:\n",
    "        files_missing = [name for name, keys in key_sets.items() if key not in keys]\n",
    "        if files_missing:\n",
    "            print(f\"Key '{key}' missing in {len(files_missing)} files: {', '.join(files_missing[:3])}{'...' if len(files_missing) > 3 else ''}\")\n",
    "        else:\n",
    "            print(f\"Key '{key}' present in all {len(pt_files)} files\")\n",
    "\n",
    "pt_file = None\n",
    "data_dir = r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\win_tensors_balanced\"\n",
    "if pt_file:\n",
    "    pt_path = Path(pt_file)\n",
    "    if not pt_path.exists():\n",
    "        print(f\"File not found: {pt_path}\")\n",
    "\n",
    "    inspect_pt_file(pt_path)\n",
    "elif data_dir:\n",
    "    data_dir = Path(data_dir)\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Directory not found: {data_dir}\")\n",
    "\n",
    "    scan_directory(data_dir)\n",
    "else:\n",
    "    print(\"Please provide --pt-file or --data-dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcc3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import logging\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s][%(levelname)s] %(message)s\")\n",
    "\n",
    "def print_tensor_info(name: str, tensor: Any) -> None:\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        logging.info(f\"  {name}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\")\n",
    "    else:\n",
    "        logging.info(f\"  {name}: {tensor} (type={type(tensor).__name__})\")\n",
    "\n",
    "def inspect_meta(pt_path: Path) -> None:\n",
    "    logging.info(f\"Inspecting: {pt_path.name}\")\n",
    "    try:\n",
    "        obj = torch.load(pt_path, map_location=\"cpu\")\n",
    "        logging.info(f\"Top-level keys: {list(obj.keys())}\")\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            meta = obj.get(\"meta\", {})\n",
    "            if isinstance(meta, dict):\n",
    "                logging.info(\"Contents of 'meta':\")\n",
    "                for key, value in meta.items():\n",
    "                    if isinstance(value, dict):\n",
    "                        logging.info(f\"  {key}: dict with {len(value)} keys (e.g., {list(value.keys())[:3]}{'...' if len(value) > 3 else ''})\")\n",
    "                    else:\n",
    "                        print_tensor_info(key, value)\n",
    "            for key, value in obj.items():\n",
    "                if key != \"meta\":\n",
    "                    print_tensor_info(key, value)\n",
    "        else:\n",
    "            logging.info(\"Expected dict, got PyG Data or other type\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {pt_path.name}: {str(e)}\")\n",
    "\n",
    "def test_batching(data_dir: Path, batch_size: int = 2) -> None:\n",
    "    class TempDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, data_dir: Path):\n",
    "            self.files = sorted([p for p in data_dir.glob(\"*.pt\") if p.is_file()])\n",
    "            if not self.files:\n",
    "                raise ValueError(f\"No .pt files in {data_dir}\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            return min(len(self.files), 10)  # Test first 10 files\n",
    "        \n",
    "        def __getitem__(self, i: int) -> Data:\n",
    "            obj = torch.load(self.files[i], map_location=\"cpu\")\n",
    "            lab = obj.get(\"label\", obj.get(\"y\", None))\n",
    "            if lab is None:\n",
    "                raise KeyError(f\"{self.files[i].name}: missing 'label'/'y'\")\n",
    "            d = Data(x=obj[\"x\"], edge_index=obj[\"edge_index\"], edge_attr=obj[\"edge_attr\"], y=torch.tensor(int(lab)))\n",
    "            d.x = d.x.to(torch.float32)\n",
    "            d.edge_index = d.edge_index.to(torch.long)\n",
    "            d.edge_attr = d.edge_attr.to(torch.float32)\n",
    "            d.y = d.y.view(-1)[0].to(torch.long)\n",
    "            return d\n",
    "\n",
    "    logging.info(f\"Testing batching on {data_dir}\")\n",
    "    try:\n",
    "        dataset = TempDataset(data_dir)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        batch = next(iter(loader))\n",
    "        logging.info(\"Batching successful! Batch details:\")\n",
    "        print_tensor_info(\"x\", batch.x)\n",
    "        print_tensor_info(\"edge_index\", batch.edge_index)\n",
    "        print_tensor_info(\"edge_attr\", batch.edge_attr)\n",
    "        print_tensor_info(\"y\", batch.y)\n",
    "        print_tensor_info(\"batch\", batch.batch)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Batching failed: {str(e)}\")\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "pt_file = None\n",
    "data_dir = r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\win_tensors_balanced\"\n",
    "if pt_file:\n",
    "    pt_path = Path(pt_file)\n",
    "    if not pt_path.exists():\n",
    "        logging.error(f\"File not found: {pt_path}\")\n",
    "\n",
    "    inspect_meta(pt_path)\n",
    "if data_dir:\n",
    "    data_dir = Path(data_dir)\n",
    "    if not data_dir.exists():\n",
    "        logging.error(f\"Directory not found: {data_dir}\")\n",
    "\n",
    "    inspect_meta(data_dir / sorted(data_dir.glob(\"*.pt\"))[0])  # Inspect first file\n",
    "    test_batching(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e053481",
   "metadata": {},
   "source": [
    "## Check single .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import glob\n",
    "\n",
    "def check_pt_file(file_path):\n",
    "    \"\"\"Inspect a single .pt file and print its contents.\"\"\"\n",
    "    print(f\"\\nChecking file: {file_path}\")\n",
    "    try:\n",
    "        # Attempt to load the .pt file\n",
    "        data = torch.load(file_path)\n",
    "        print(f\"  File loaded successfully!\")\n",
    "        print(f\"  Type of loaded object: {type(data)}\")\n",
    "\n",
    "        # Check if it's a PyTorch Geometric Data object\n",
    "        if isinstance(data, Data):\n",
    "            print(\"  PyG Data object detected with attributes:\")\n",
    "            for key in data.keys:\n",
    "                value = data[key]\n",
    "                print(f\"    {key}: {type(value)}, Shape: {getattr(value, 'shape', 'N/A')}\")\n",
    "            if hasattr(data, 'y'):\n",
    "                print(f\"    Label (y): {data.y}\")\n",
    "        # Check if it's a tensor\n",
    "        elif isinstance(data, torch.Tensor):\n",
    "            print(f\"  Tensor detected with shape: {data.shape}\")\n",
    "            print(f\"  Data type: {data.dtype}\")\n",
    "        # Check if it's a dictionary or other object\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"  Dictionary detected with keys:\")\n",
    "            for key, value in data.items():\n",
    "                print(f\"    {key}: {type(value)}, Shape: {getattr(value, 'shape', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"  Unknown object type: {type(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading file: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Path to the directory containing .pt files\n",
    "    directory = \"./win_tensors_balanced\"  # Adjust if your files are elsewhere\n",
    "    pt_files = glob.glob(os.path.join(directory, \"*.pt\"))\n",
    "\n",
    "    if not pt_files:\n",
    "        print(f\"No .pt files found in {directory}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(pt_files)} .pt files\")\n",
    "    for file_path in pt_files:\n",
    "        check_pt_file(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886595c",
   "metadata": {},
   "source": [
    "## Checking GINE Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def check_gine_compatibility(file_path):\n",
    "    \"\"\"Check if a .pt file is compatible with GINE model requirements.\"\"\"\n",
    "    print(f\"\\nChecking GINE compatibility for: {file_path}\")\n",
    "    try:\n",
    "        data = torch.load(file_path)\n",
    "        if not isinstance(data, Data):\n",
    "            print(\"  Error: Not a PyTorch Geometric Data object\")\n",
    "            return\n",
    "\n",
    "        # Check required attributes for GINE\n",
    "        required_attrs = ['x', 'edge_index', 'edge_attr', 'y']\n",
    "        missing_attrs = [attr for attr in required_attrs if not hasattr(data, attr)]\n",
    "        if missing_attrs:\n",
    "            print(f\"  Missing attributes: {missing_attrs}\")\n",
    "        else:\n",
    "            print(\"  All required attributes present:\")\n",
    "            print(f\"    Node features (x): {data.x.shape}, Type: {data.x.dtype}\")\n",
    "            print(f\"    Edge index (edge_index): {data.edge_index.shape}, Type: {data.edge_index.dtype}\")\n",
    "            print(f\"    Edge attributes (edge_attr): {data.edge_attr.shape}, Type: {data.edge_attr.dtype}\")\n",
    "            print(f\"    Label (y): {data.y}, Type: {data.y.dtype}\")\n",
    "\n",
    "            # Verify data integrity\n",
    "            if data.x.shape[0] == 0:\n",
    "                print(\"  Warning: No nodes (x.shape[0] == 0)\")\n",
    "            if data.edge_index.shape[1] == 0:\n",
    "                print(\"  Warning: No edges (edge_index.shape[1] == 0)\")\n",
    "            if data.edge_index.max() >= data.x.shape[0]:\n",
    "                print(\"  Error: Edge indices reference non-existent nodes\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading file: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Path to a single .pt file for testing (replace with a problematic file)\n",
    "    file_path = r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\win_tensors_balanced\\win_2019-05-07T18-40-00Z__2019-05-07T18-50-00Z__ben_non_malicious__ta1-marple-2-e5-official-1.bin__part-000003.pt\"\n",
    "    check_gine_compatibility(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\win_tensors_balanced\\win_2019-05-07T18-40-00Z__2019-05-07T18-50-00Z__ben_non_malicious__ta1-marple-2-e5-official-1.bin__part-000003.pt\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa4493",
   "metadata": {},
   "source": [
    "## Checking Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2259b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"[%(asctime)s][%(levelname)s] %(message)s\",\n",
    "        handlers=[logging.StreamHandler()]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "def analyze_dataset(data_dir: str):\n",
    "    pt_files = glob.glob(os.path.join(data_dir, \"*.pt\"))\n",
    "    if not pt_files:\n",
    "        logger.error(f\"No .pt files found in {data_dir}\")\n",
    "        return\n",
    "\n",
    "    labels = []\n",
    "    node_counts = []\n",
    "    edge_counts = []\n",
    "    x_stats = []\n",
    "    edge_attr_stats = []\n",
    "\n",
    "    for file_path in pt_files:\n",
    "        try:\n",
    "            loaded_data = torch.load(file_path)\n",
    "            if not isinstance(loaded_data, dict):\n",
    "                logger.warning(f\"Skipping {file_path}: Not a dict\")\n",
    "                continue\n",
    "\n",
    "            x = loaded_data.get('x')\n",
    "            edge_index = loaded_data.get('edge_index')\n",
    "            edge_attr = loaded_data.get('edge_attr')\n",
    "            label = loaded_data.get('label')\n",
    "\n",
    "            if x is None or edge_index is None or edge_attr is None or label is None:\n",
    "                logger.warning(f\"Skipping {file_path}: Missing required keys\")\n",
    "                continue\n",
    "\n",
    "            labels.append(int(label))\n",
    "            node_counts.append(x.shape[0])\n",
    "            edge_counts.append(edge_index.shape[1])\n",
    "            x_stats.append([x.min().item(), x.max().item(), x.mean().item(), x.std().item()])\n",
    "            edge_attr_stats.append([edge_attr.min().item(), edge_attr.max().item(), edge_attr.mean().item(), edge_attr.std().item()])\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {file_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not labels:\n",
    "        logger.error(\"No valid data found\")\n",
    "        return\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    class_counts = np.bincount(labels)\n",
    "    logger.info(f\"Class distribution: {class_counts}\")\n",
    "    logger.info(f\"Class weights: {1.0 / (class_counts + 1e-6)}\")\n",
    "\n",
    "    node_counts = np.array(node_counts)\n",
    "    edge_counts = np.array(edge_counts)\n",
    "    x_stats = np.array(x_stats)\n",
    "    edge_attr_stats = np.array(edge_attr_stats)\n",
    "\n",
    "    logger.info(f\"Node counts: min={node_counts.min()}, max={node_counts.max()}, mean={node_counts.mean():.2f}, std={node_counts.std():.2f}\")\n",
    "    logger.info(f\"Edge counts: min={edge_counts.min()}, max={edge_counts.max()}, mean={edge_counts.mean():.2f}, std={edge_counts.std():.2f}\")\n",
    "    logger.info(f\"Node features (x): min={x_stats[:,0].min():.4f}, max={x_stats[:,1].max():.4f}, mean={x_stats[:,2].mean():.4f}, std={x_stats[:,3].mean():.4f}\")\n",
    "    logger.info(f\"Edge features (edge_attr): min={edge_attr_stats[:,0].min():.4f}, max={edge_attr_stats[:,1].max():.4f}, mean={edge_attr_stats[:,2].mean():.4f}, std={edge_attr_stats[:,3].mean():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = r\"C:\\Users\\Ali\\Desktop\\ChatGPT Scripts\\win_tensors_balanced\"\n",
    "    analyze_dataset(data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
